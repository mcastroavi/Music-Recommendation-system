{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb7278-8a48-4a52-8537-72d4f0eb017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code implements a machine learning pipeline for predictive modeling, primarily using Gradient Boosting Classification. Here's a breakdown of the key techniques:\n",
    "# 1. Data Loading & Preprocessing\n",
    "# - Reads data from .txt files and converts them into pandas DataFrames.\n",
    "# - Cleans userID and trackID by converting them to strings and stripping whitespace.\n",
    "# 2. Feature Engineering\n",
    "# - Total Score Calculation: Combines albumScore and artistScore to create totalScore.\n",
    "# - Normalization (Z-score scaling): Standardizes features within each userID group to make values more comparable.\n",
    "# - Interaction Features:\n",
    "# - album_x_artist: Interaction term by multiplying albumScore and artistScore.\n",
    "# - album_div_artist: Division-based interaction to assess relative importance.\n",
    "# - Ranking: Assigns ranks to totalScore within each userID group.\n",
    "# 3. Popularity & User Activity Features\n",
    "# - Computes track popularity as the mean totalScore for each trackID.\n",
    "# - Calculates track rating count to measure how many times a track has been rated.\n",
    "# - Determines user average score to reflect overall scoring behavior.\n",
    "# 4. Model Training & Optimization\n",
    "# - Uses Gradient Boosting Classifier, an ensemble method that builds weak learners sequentially to improve performance.\n",
    "# - Hyperparameters:\n",
    "# - n_estimators=200: Number of trees in the ensemble.\n",
    "# - learning_rate=0.01: Controls contribution of each tree to prevent overfitting.\n",
    "# - max_depth=5: Limits tree depth to manage complexity.\n",
    "# - subsample=0.65: Uses a fraction of data per iteration for regularization.\n",
    "# - Implements Cross-Validation to evaluate model performance.\n",
    "# This pipeline combines data preprocessing, feature engineering, and model tuning to maximize prediction accuracy (~88%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18eb14-5b37-4791-9ac4-52f3eb7a7325",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0.880 accuracy \n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "# === Load Data ===\n",
    "data_file = Path('Data_matrix_UserID_TrackID_Score.txt') ## add path\n",
    "ground_truth_file = Path('test2_new.txt') ## add path\n",
    "\n",
    "# Load the datasets\n",
    "df = pd.read_csv(data_file, sep='|', header=None, names=['userID', 'trackID', 'albumScore', 'artistScore'])\n",
    "gt = pd.read_csv(ground_truth_file, sep='|', header=None, names=['userID', 'trackID', 'prediction'])\n",
    "\n",
    "# Convert userID and trackID to strings and remove any leading/trailing whitespace\n",
    "for col in ['userID', 'trackID']:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    gt[col] = gt[col].astype(str).str.strip()\n",
    "\n",
    "# Calculate the total score\n",
    "df['totalScore'] = df['albumScore'] + df['artistScore']\n",
    "\n",
    "# Merge the data with the ground truth to get the training data\n",
    "train_df = pd.merge(df, gt, on=['userID', 'trackID'])\n",
    "\n",
    "# === Normalize Features ===\n",
    "def zscore(x):\n",
    "    return (x - x.mean()) / (x.std() + 1e-8)\n",
    "\n",
    "for col in ['albumScore', 'artistScore', 'totalScore']:\n",
    "    df[f'{col}_norm'] = df.groupby('userID')[col].transform(zscore)\n",
    "    train_df[f'{col}_norm'] = train_df.groupby('userID')[col].transform(zscore)\n",
    "\n",
    "# === Interaction Features ===\n",
    "df['album_x_artist'] = df['albumScore'] * df['artistScore']\n",
    "df['album_div_artist'] = df['albumScore'] / (df['artistScore'] + 1e-5)\n",
    "train_df['album_x_artist'] = train_df['albumScore'] * train_df['artistScore']\n",
    "train_df['album_div_artist'] = train_df['albumScore'] / (train_df['artistScore'] + 1e-5)\n",
    "\n",
    "# Rank scores within each user group\n",
    "df['score_rank'] = df.groupby('userID')['totalScore'].rank(ascending=False)\n",
    "train_df['score_rank'] = train_df.groupby('userID')['totalScore'].rank(ascending=False)\n",
    "\n",
    "# === Global Popularity & User Activity ===\n",
    "track_pop = df.groupby('trackID')['totalScore'].mean().rename('track_popularity')\n",
    "track_count = df.groupby('trackID').size().rename('track_rating_count')\n",
    "user_avg = df.groupby('userID')['totalScore'].mean().rename('user_avg_score')\n",
    "\n",
    "# Merge the popularity and activity features into the main DataFrame\n",
    "df = df.merge(track_pop, on='trackID', how='left')\n",
    "df = df.merge(track_count, on='trackID', how='left')\n",
    "df = df.merge(user_avg, on='userID', how='left')\n",
    "\n",
    "train_df = train_df.merge(track_pop, on='trackID', how='left')\n",
    "train_df = train_df.merge(track_count, on='trackID', how='left')\n",
    "train_df = train_df.merge(user_avg, on='userID', how='left')\n",
    "\n",
    "# === Final Feature List ===\n",
    "features = [\n",
    "    'albumScore', 'artistScore', 'totalScore',\n",
    "    'albumScore_norm', 'artistScore_norm', 'totalScore_norm',\n",
    "    'album_x_artist', 'album_div_artist',\n",
    "    'score_rank', 'track_popularity',\n",
    "    'track_rating_count', 'user_avg_score'\n",
    "]\n",
    "\n",
    "X = train_df[features]\n",
    "y = train_df['prediction'].astype(int)\n",
    "\n",
    "# ####Tunning this part improve or decrease score ################ With the actual numbers the score is 0.880\n",
    "# === Cross-Validation ===\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.65,\n",
    "    random_state=0\n",
    ")\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10364af6-2dd7-4f6c-a50a-30cc50ce873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# === Train-Test Split for Final Evaluation ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Predict (pure .predict(), no ranking logic) ===\n",
    "df['predicted'] = model.predict(df[features])\n",
    "# === Evaluate ===\n",
    "eval_df = pd.merge(df, gt, on=['userID', 'trackID'], suffixes=('_pred', '_true'))\n",
    "print(\"âœ… Accuracy:\", accuracy_score(eval_df['prediction'], eval_df['predicted']))\n",
    "print(\"ðŸ“Š Classification Report:\\n\", classification_report(eval_df['prediction'], eval_df['predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6107e-6058-467f-9684-bdafa4780b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final Submission ===\n",
    "df['trackID_combined'] = df['userID'] + '_' + df['trackID']\n",
    "submission = df[['trackID_combined', 'predicted']]\n",
    "submission.columns = ['trackID', 'predictor']\n",
    "submission.to_csv('submission_final_featureboost_v14c.csv', index=False) ## add path\n",
    "\n",
    "# === Compare with Ground Truth for Accuracy ===\n",
    "# Create a combined column in the ground truth DataFrame\n",
    "gt['trackID_combined'] = gt['userID'] + '_' + gt['trackID']\n",
    "\n",
    "# Merge the final submission with the ground truth\n",
    "ground_truth_comparison = pd.merge(submission, gt, left_on='trackID', right_on='trackID_combined', how='inner')\n",
    "ground_truth_comparison['correct'] = ground_truth_comparison['predictor'] == ground_truth_comparison['prediction']\n",
    "final_accuracy = ground_truth_comparison['correct'].mean()\n",
    "\n",
    "print(f\"Final accuracy compared to ground truth: {final_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
